{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain + GPT3.5 For Dependency Extraction\n",
    "First we are going to run this on a small subset of Flink as the whole thing will cost a lot of money.  We will start with a simple reduced structure found in /flink-reduced\n",
    "\n",
    "### Step 1: Download reqs, load OPENAI_API_KEY from env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip install openai tiktoken chromadb langchain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13218"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import os \n",
    "import re\n",
    "path = \".\\\\flink-1.17.1\"\n",
    "\n",
    "def remove_comments(code):\n",
    "    # Removing /* ... */ comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    # Removing // comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    return code\n",
    "\n",
    "def read_file(file_path):\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    return f.read()\n",
    "  \n",
    "def load_documents():\n",
    "  documents = []\n",
    "  encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "  for dir_path, _, file_names in os.walk(path):\n",
    "    for file_name in file_names:\n",
    "      if file_name.endswith(\".java\"):\n",
    "        content = read_file(dir_path + \"\\\\\" + file_name)\n",
    "        content = remove_comments(content)\n",
    "\n",
    "        while len(encoding.encode(content)) > 600:\n",
    "          content = content[:min(800, len(content))]\n",
    "          \n",
    "        documents.append({\"file_path\": dir_path + \"\\\\\" + file_name, \"source\": content})\n",
    "  return documents\n",
    "\n",
    "\n",
    "documents = load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\FlinkVersion.java', 'source': '\\n\\npackage org.apache.flink;\\n\\nimport org.apache.flink.annotation.Public;\\n\\nimport java.util.Arrays;\\nimport java.util.LinkedHashSet;\\nimport java.util.Map;\\nimport java.util.Optional;\\nimport java.util.Set;\\nimport java.util.function.Function;\\nimport java.util.stream.Collectors;\\nimport java.util.stream.Stream;\\n\\n\\n@Public\\npublic enum FlinkVersion {\\n\\n    \\n    \\n    \\n    v1_3(\"1.3\"),\\n    v1_4(\"1.4\"),\\n    v1_5(\"1.5\"),\\n    v1_6(\"1.6\"),\\n    v1_7(\"1.7\"),\\n    v1_8(\"1.8\"),\\n    v1_9(\"1.9\"),\\n    v1_10(\"1.10\"),\\n    v1_11(\"1.11\"),\\n    v1_12(\"1.12\"),\\n    v1_13(\"1.13\"),\\n    v1_14(\"1.14\"),\\n    v1_15(\"1.15\"),\\n    v1_16(\"1.16\"),\\n    v1_17(\"1.17\");\\n\\n    private final String versionStr;\\n\\n    FlinkVersion(String versionStr) {\\n        this.versionStr = versionStr;\\n    }\\n\\n    @Override\\n    public String toString() {\\n        return versionStr;\\n    }\\n\\n    public boolean isNewerVersionThan(FlinkVersion otherVersion) {\\n        return this.ordinal() > otherVersion.ordinal();\\n    }\\n\\n    \\n    public static Set<FlinkVersion> rangeOf(FlinkVersion start, FlinkVersion end) {\\n        return Stream.of(FlinkVersion.values())\\n                .filter(v -> v.ordinal() >= start.ordinal() && v.ordinal() <= end.ordinal())\\n                .collect(Collectors.toCollection(LinkedHashSet::new));\\n    }\\n\\n    private static final Map<String, FlinkVersion> CODE_MAP =\\n            Arrays.stream(values())\\n                    .collect(Collectors.toMap(v -> v.versionStr, Function.identity()));\\n\\n    public static Optional<FlinkVersion> byCode(String code) {\\n        return Optional.ofNullable(CODE_MAP.get(code));\\n    }\\n\\n    \\n    public static FlinkVersion current() {\\n        return values()[values().length - 1];\\n    }\\n}\\n'}\n",
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\annotation\\\\Experimental.java', 'source': '\\n\\npackage org.apache.flink.annotation;\\n\\nimport java.lang.annotation.Documented;\\nimport java.lang.annotation.ElementType;\\nimport java.lang.annotation.Target;\\n\\n\\n@Documented\\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR})\\n@Public\\npublic @interface Experimental {}\\n'}\n",
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\annotation\\\\Internal.java', 'source': '\\n\\npackage org.apache.flink.annotation;\\n\\nimport java.lang.annotation.Documented;\\nimport java.lang.annotation.ElementType;\\nimport java.lang.annotation.Target;\\n\\n\\n@Documented\\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.CONSTRUCTOR, ElementType.FIELD})\\n@Public\\npublic @interface Internal {}\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])\n",
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "   \n",
    "initial_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Instruction: You will only return valid JSON. Given the following code, extract any internal dependencies. \n",
    "    Output must be a valid JSON array of strings. For the given code you must \n",
    "    determine what external files or packages it depends on, and return them.              \n",
    "    File Path: {file_path}\n",
    "    Code: {source} \n",
    "    Answer in valid JSON: \\n\\n###\\n\\n\"\"\",\n",
    "    input_variables=[\"file_path\", \"source\"]\n",
    ")\n",
    "\n",
    "retry_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    The previous response, \"{response}\" was not valid JSON. Please try again. \n",
    "    Instruction: You will only return valid JSON. Given the following code, extract any internal dependencies. \n",
    "    Output must be a valid JSON array of strings. For the given code you must \n",
    "    determine what external files or packages it depends on, and return them.                    \n",
    "    File Path: {file_path}\n",
    "    Code: {source} \n",
    "    Answer in valid, unformatted JSON: \\n\\n###\\n\\n\"\"\",\n",
    "    input_variables=[\"response\", \"file_path\", \"source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define response extraction stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "output = []\n",
    "parse_fails = 0\n",
    "retry_fails = 0\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\", frequency_penalty=0, presence_penalty=0, top_p=0.9, max_tokens=1000)\n",
    "\n",
    "def invoke_model(document, prev_response=None):\n",
    "    file_path = document[\"file_path\"]\n",
    "    source = document[\"source\"]\n",
    "    is_retry = prev_response is not None\n",
    "\n",
    "    if is_retry:\n",
    "        formatted = retry_prompt.format(file_path=file_path, source=source, response=prev_response)\n",
    "    else: \n",
    "        formatted = initial_prompt.format(file_path=file_path, source=source)\n",
    "\n",
    "    response = llm.invoke(formatted)\n",
    "    response = response.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    success = handle_response(file_path, response, is_retry)\n",
    "    if success or (not success and is_retry): \n",
    "        return\n",
    "    else:\n",
    "        invoke_model(document, prev_response=response)\n",
    "\n",
    "            \n",
    "def handle_response(file_path, response, is_retry):\n",
    "    buffered_output = []\n",
    "    try: \n",
    "        parsed = json.loads(response)\n",
    "\n",
    "        for dependency in parsed:\n",
    "            if isinstance(dependency, str):\n",
    "                buffered_output.append((file_path, dependency))\n",
    "            else:\n",
    "                handle_parse_fail(response, \"Dependency was not a string\", is_retry)\n",
    "                return False\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        handle_parse_fail(response, \"Invalid JSON\", is_retry)\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        for item in buffered_output:\n",
    "            output.append(item)\n",
    "        return True\n",
    "\n",
    "\n",
    "def handle_parse_fail(response, msg, is_retry):\n",
    "    global retry_fails, parse_fails\n",
    "\n",
    "    if is_retry:\n",
    "        retry_fails += 1\n",
    "        msg_type = \"Retry\"\n",
    "    else:\n",
    "        parse_fails += 1\n",
    "        msg_type = \"Parse\"\n",
    "        \n",
    "    print(f\"{msg_type} fail ({msg}): {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Process in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54822\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(f\"bin/deps_4000_5000.pkl\", \"rb\") as f:\n",
    "    output = pickle.load(f)\n",
    "\n",
    "print(len(output))\n",
    "batch_start = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch start: 13219\n",
      "Batch size: 2\n",
      "Parse fails: 311\n",
      "Retry fails: 0\n",
      "Output length: 126518\n",
      "Saved output\n",
      "Start for next run is 13221\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "batch_size = 2\n",
    "batch_end = batch_start + batch_size\n",
    "batch = documents[batch_start:batch_end]\n",
    "\n",
    "for i, document in enumerate(batch):\n",
    "    try:\n",
    "        invoke_model(document)\n",
    "    except Exception as e:\n",
    "        modified_doc = document[\"source\"][:len(document[\"source\"]) // 4]\n",
    "        print(\"api err 1x\")\n",
    "        try:\n",
    "            invoke_model(modified_doc)\n",
    "        except Exception as e:\n",
    "            print(\"api err 2x\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Iteration: {i}\")\n",
    "        \n",
    "batch_start = batch_end\n",
    "\n",
    "print(f\"Batch start: {batch_end - batch_size}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Parse fails: {parse_fails}\")\n",
    "print(f\"Retry fails: {retry_fails}\")\n",
    "print(f\"Output length: {len(output)}\")\n",
    "\n",
    "with open(f\"bin/deps_{batch_end - batch_size}_{batch_end}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output, f)\n",
    "    print(\"Saved output\")\n",
    "\n",
    "print(f\"Start for next run is {batch_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\flink-1.17.1\\flink-runtime-web\\src\\main\\java\\org\\apache\\flink\\runtime\\webmonitor\\handlers\\JarRunMessageParameters.java org.apache.flink.runtime.rest.messages.MessageParameters\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-hive\\src\\test\\java\\org\\apache\\flink\\connectors\\hive\\read\\HivePartitionFetcherTest.java org.apache.flink.connectors.hive.HiveOptions\n",
      ".\\flink-1.17.1\\flink-yarn-tests\\src\\test\\java\\org\\apache\\flink\\yarn\\YARNSessionCapacitySchedulerITCase.java org.apache.flink.runtime.testutils.CommonTestUtils\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\main\\java\\org\\apache\\flink\\runtime\\operators\\coordination\\CoordinationResponse.java java.io.Serializable\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-files\\src\\main\\java\\org\\apache\\flink\\connector\\file\\table\\stream\\ProcTimeCommitTrigger.java org.apache.flink.streaming.runtime.tasks.ProcessingTimeService\n",
      ".\\flink-1.17.1\\flink-clients\\src\\main\\java\\org\\apache\\flink\\client\\program\\MiniClusterClient.java org.apache.flink.util.SerializedValue\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-files\\src\\test\\java\\org\\apache\\flink\\connector\\file\\sink\\FileSinkCompactionSwitchITCase.java org.apache.flink.runtime.minicluster.RpcServiceSharing\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-base\\src\\test\\java\\org\\apache\\flink\\connector\\base\\source\\reader\\SourceReaderBaseTest.java org.apache.flink.api.connector.source.mocks.MockSourceSplit\n",
      ".\\flink-1.17.1\\flink-streaming-java\\src\\test\\java\\org\\apache\\flink\\streaming\\runtime\\watermarkstatus\\HeapPriorityQueueTest.java org.junit.jupiter.api.Test\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-planner\\src\\test\\java\\org\\apache\\flink\\table\\planner\\plan\\nodes\\exec\\serde\\RankRangeSerdeTest.java org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_out = random.sample(output, 10)\n",
    "\n",
    "for x,y in rand_out:\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse fails: 311\n",
      "Retry fails: 0\n",
      "Output length: 126518\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parse fails: {parse_fails}\")\n",
    "print(f\"Retry fails: {retry_fails}\")\n",
    "print(f\"Output length: {len(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Clean up, write to TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed size: 120269\n"
     ]
    }
   ],
   "source": [
    "def package_to_path(source, package_name):\n",
    "    # Extract the base directory up to and including 'java'\n",
    "    base_dir = source.split(\"\\\\java\\\\\")[0] + \"\\\\java\\\\\"\n",
    "\n",
    "    # Convert the package name to a path and append to base_dir\n",
    "    package_path = package_name.replace(\".\", \"\\\\\") + \".java\"\n",
    "    return base_dir + package_path\n",
    "\n",
    "processed_output = set([(source.replace(\"\\\\\", \"/\"), package_to_path(source, dependency).replace(\"\\\\\", \"/\")) for source, dependency in output])\n",
    "\n",
    "print(f\"Processed size: {len(processed_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ta_output = \"./source_raw_ta/llm_dependencies.raw.ta\"\n",
    "\n",
    "with open(raw_ta_output, \"w+\") as f:\n",
    "  f.write(\"FACT TUPLE : \\n\")\n",
    "\n",
    "  unique_file_paths = set(file_path for file_path, _ in processed_output)\n",
    "\n",
    "  # first generate all the concrete instances\n",
    "  for file_path in unique_file_paths:\n",
    "    f.write(f\"$INSTANCE {file_path} cFile\\n\")\n",
    "\n",
    "  # now add in all the dependencies\n",
    "  for file_path, dependency in processed_output:\n",
    "    f.write(f\"cLinks {file_path} {dependency}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
