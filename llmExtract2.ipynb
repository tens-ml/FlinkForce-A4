{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain + GPT3.5 For Dependency Extraction\n",
    "First we are going to run this on a small subset of Flink as the whole thing will cost a lot of money.  We will start with a simple reduced structure found in /flink-reduced\n",
    "\n",
    "### Step 1: Download reqs, load OPENAI_API_KEY from env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip install openai tiktoken chromadb langchain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\k899e\\projects\\school\\4314\\a4\\llmExtract2.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         documents\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mfile_path\u001b[39m\u001b[39m\"\u001b[39m: dir_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m file_name, \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: content})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m documents\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m documents \u001b[39m=\u001b[39m load_documents()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mlen\u001b[39m(documents)\n",
      "\u001b[1;32mc:\\Users\\k899e\\projects\\school\\4314\\a4\\llmExtract2.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_name \u001b[39min\u001b[39;00m file_names:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mif\u001b[39;00m file_name\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.java\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     content \u001b[39m=\u001b[39m read_file(dir_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m file_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     content \u001b[39m=\u001b[39m remove_comments(content)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(encoding\u001b[39m.\u001b[39mencode(content)) \u001b[39m>\u001b[39m \u001b[39m600\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\k899e\\projects\\school\\4314\\a4\\llmExtract2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_file\u001b[39m(file_path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/k899e/projects/school/4314/a4/llmExtract2.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[39m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import os \n",
    "import re\n",
    "path = \".\\\\flink-1.17.1\"\n",
    "\n",
    "def remove_comments(code):\n",
    "    # Removing /* ... */ comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    # Removing // comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    return code\n",
    "\n",
    "def read_file(file_path):\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    return f.read()\n",
    "  \n",
    "def load_documents():\n",
    "  documents = []\n",
    "  encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "  for dir_path, _, file_names in os.walk(path):\n",
    "    for file_name in file_names:\n",
    "      if file_name.endswith(\".java\"):\n",
    "        content = read_file(dir_path + \"\\\\\" + file_name)\n",
    "        content = remove_comments(content)\n",
    "\n",
    "        while len(encoding.encode(content)) > 600:\n",
    "          content = content[:min(800, len(content))]\n",
    "          \n",
    "        documents.append({\"file_path\": dir_path + \"\\\\\" + file_name, \"source\": content})\n",
    "  return documents\n",
    "\n",
    "\n",
    "documents = load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\FlinkVersion.java', 'source': '\\n\\npackage org.apache.flink;\\n\\nimport org.apache.flink.annotation.Public;\\n\\nimport java.util.Arrays;\\nimport java.util.LinkedHashSet;\\nimport java.util.Map;\\nimport java.util.Optional;\\nimport java.util.Set;\\nimport java.util.function.Function;\\nimport java.util.stream.Collectors;\\nimport java.util.stream.Stream;\\n\\n\\n@Public\\npublic enum FlinkVersion {\\n\\n    \\n    \\n    \\n    v1_3(\"1.3\"),\\n    v1_4(\"1.4\"),\\n    v1_5(\"1.5\"),\\n    v1_6(\"1.6\"),\\n    v1_7(\"1.7\"),\\n    v1_8(\"1.8\"),\\n    v1_9(\"1.9\"),\\n    v1_10(\"1.10\"),\\n    v1_11(\"1.11\"),\\n    v1_12(\"1.12\"),\\n    v1_13(\"1.13\"),\\n    v1_14(\"1.14\"),\\n    v1_15(\"1.15\"),\\n    v1_16(\"1.16\"),\\n    v1_17(\"1.17\");\\n\\n    private final String versionStr;\\n\\n    FlinkVersion(String versionStr) {\\n        this.versionStr = versionStr;\\n    }\\n\\n    @Override\\n    public String toString() {\\n        return versionStr;\\n    }\\n\\n    public boolean isNewerVersionThan(FlinkVersion otherVersion) {\\n        return this.ordinal() > otherVersion.ordinal();\\n    }\\n\\n    \\n    public static Set<FlinkVersion> rangeOf(FlinkVersion start, FlinkVersion end) {\\n        return Stream.of(FlinkVersion.values())\\n                .filter(v -> v.ordinal() >= start.ordinal() && v.ordinal() <= end.ordinal())\\n                .collect(Collectors.toCollection(LinkedHashSet::new));\\n    }\\n\\n    private static final Map<String, FlinkVersion> CODE_MAP =\\n            Arrays.stream(values())\\n                    .collect(Collectors.toMap(v -> v.versionStr, Function.identity()));\\n\\n    public static Optional<FlinkVersion> byCode(String code) {\\n        return Optional.ofNullable(CODE_MAP.get(code));\\n    }\\n\\n    \\n    public static FlinkVersion current() {\\n        return values()[values().length - 1];\\n    }\\n}\\n'}\n",
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\annotation\\\\Experimental.java', 'source': '\\n\\npackage org.apache.flink.annotation;\\n\\nimport java.lang.annotation.Documented;\\nimport java.lang.annotation.ElementType;\\nimport java.lang.annotation.Target;\\n\\n\\n@Documented\\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR})\\n@Public\\npublic @interface Experimental {}\\n'}\n",
      "{'file_path': '.\\\\flink-1.17.1\\\\flink-annotations\\\\src\\\\main\\\\java\\\\org\\\\apache\\\\flink\\\\annotation\\\\Internal.java', 'source': '\\n\\npackage org.apache.flink.annotation;\\n\\nimport java.lang.annotation.Documented;\\nimport java.lang.annotation.ElementType;\\nimport java.lang.annotation.Target;\\n\\n\\n@Documented\\n@Target({ElementType.TYPE, ElementType.METHOD, ElementType.CONSTRUCTOR, ElementType.FIELD})\\n@Public\\npublic @interface Internal {}\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])\n",
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "   \n",
    "initial_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Instruction: You will only return valid JSON. Given the following code, extract any internal dependencies. \n",
    "    Output must be a valid JSON array of strings. For the given code you must \n",
    "    determine what external files or packages it depends on, and return them.              \n",
    "    File Path: {file_path}\n",
    "    Code: {source} \n",
    "    Answer in valid JSON: \\n\\n###\\n\\n\"\"\",\n",
    "    input_variables=[\"file_path\", \"source\"]\n",
    ")\n",
    "\n",
    "retry_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    The previous response, \"{response}\" was not valid JSON. Please try again. \n",
    "    Instruction: You will only return valid JSON. Given the following code, extract any internal dependencies. \n",
    "    Output must be a valid JSON array of strings. For the given code you must \n",
    "    determine what external files or packages it depends on, and return them.                    \n",
    "    File Path: {file_path}\n",
    "    Code: {source} \n",
    "    Answer in valid, unformatted JSON: \\n\\n###\\n\\n\"\"\",\n",
    "    input_variables=[\"response\", \"file_path\", \"source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define response extraction stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "output = []\n",
    "parse_fails = 0\n",
    "retry_fails = 0\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\", frequency_penalty=0, presence_penalty=0, top_p=0.9, max_tokens=1000)\n",
    "\n",
    "def invoke_model(document, prev_response=None):\n",
    "    file_path = document[\"file_path\"]\n",
    "    source = document[\"source\"]\n",
    "    is_retry = prev_response is not None\n",
    "\n",
    "    if is_retry:\n",
    "        formatted = retry_prompt.format(file_path=file_path, source=source, response=prev_response)\n",
    "    else: \n",
    "        formatted = initial_prompt.format(file_path=file_path, source=source)\n",
    "\n",
    "    response = llm.invoke(formatted)\n",
    "    response = response.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    success = handle_response(file_path, response, is_retry)\n",
    "    if success or (not success and is_retry): \n",
    "        return\n",
    "    else:\n",
    "        invoke_model(document, prev_response=response)\n",
    "\n",
    "            \n",
    "def handle_response(file_path, response, is_retry):\n",
    "    buffered_output = []\n",
    "    try: \n",
    "        parsed = json.loads(response)\n",
    "\n",
    "        for dependency in parsed:\n",
    "            if isinstance(dependency, str):\n",
    "                buffered_output.append((file_path, dependency))\n",
    "            else:\n",
    "                handle_parse_fail(response, \"Dependency was not a string\", is_retry)\n",
    "                return False\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        handle_parse_fail(response, \"Invalid JSON\", is_retry)\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        for item in buffered_output:\n",
    "            output.append(item)\n",
    "        return True\n",
    "\n",
    "\n",
    "def handle_parse_fail(response, msg, is_retry):\n",
    "    global retry_fails, parse_fails\n",
    "\n",
    "    if is_retry:\n",
    "        retry_fails += 1\n",
    "        msg_type = \"Retry\"\n",
    "    else:\n",
    "        parse_fails += 1\n",
    "        msg_type = \"Parse\"\n",
    "        \n",
    "    print(f\"{msg_type} fail ({msg}): {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Process in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126518\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(f\"bin/deps_13219_13221.pkl\", \"rb\") as f:\n",
    "    output = pickle.load(f)\n",
    "\n",
    "print(len(output))\n",
    "batch_start = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch start: 13219\n",
      "Batch size: 2\n",
      "Parse fails: 311\n",
      "Retry fails: 0\n",
      "Output length: 126518\n",
      "Saved output\n",
      "Start for next run is 13221\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "batch_size = 2\n",
    "batch_end = batch_start + batch_size\n",
    "batch = documents[batch_start:batch_end]\n",
    "\n",
    "for i, document in enumerate(batch):\n",
    "    try:\n",
    "        invoke_model(document)\n",
    "    except Exception as e:\n",
    "        modified_doc = document[\"source\"][:len(document[\"source\"]) // 4]\n",
    "        print(\"api err 1x\")\n",
    "        try:\n",
    "            invoke_model(modified_doc)\n",
    "        except Exception as e:\n",
    "            print(\"api err 2x\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Iteration: {i}\")\n",
    "        \n",
    "batch_start = batch_end\n",
    "\n",
    "print(f\"Batch start: {batch_end - batch_size}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Parse fails: {parse_fails}\")\n",
    "print(f\"Retry fails: {retry_fails}\")\n",
    "print(f\"Output length: {len(output)}\")\n",
    "\n",
    "with open(f\"bin/deps_{batch_end - batch_size}_{batch_end}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output, f)\n",
    "    print(\"Saved output\")\n",
    "\n",
    "print(f\"Start for next run is {batch_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-files\\src\\test\\java\\org\\apache\\flink\\connector\\file\\table\\stream\\StreamingFileWriterTest.java org.apache.flink.streaming.api.functions.sink.filesystem.OutputFileConfig\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-connector-files\\src\\main\\java\\org\\apache\\flink\\connector\\file\\table\\DefaultPartTimeExtractor.java java.time.LocalTime\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-runtime\\src\\main\\java\\org\\apache\\flink\\table\\runtime\\functions\\BuiltInSpecializedFunction.java org.apache.flink.table.catalog.DataTypeFactory\n",
      ".\\flink-1.17.1\\flink-connectors\\flink-hadoop-compatibility\\src\\test\\java\\org\\apache\\flink\\test\\hadoopcompatibility\\mapred\\HadoopMapredITCase.java org.apache.flink.test.testdata.WordCountData\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\test\\java\\org\\apache\\flink\\runtime\\checkpoint\\channel\\ChannelStateWriteRequestExecutorImplTest.java org.apache.flink.util.function.BiConsumerWithException\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-runtime\\src\\main\\java\\org\\apache\\flink\\table\\runtime\\generated\\GeneratedClass.java org.slf4j.LoggerFactory\n",
      ".\\flink-1.17.1\\flink-state-backends\\flink-statebackend-rocksdb\\src\\main\\java\\org\\apache\\flink\\contrib\\streaming\\state\\EmbeddedRocksDBStateBackend.java org.apache.flink.core.execution.SavepointFormatType\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\test\\java\\org\\apache\\flink\\runtime\\checkpoint\\StandaloneCompletedCheckpointStoreTest.java org.apache.flink.util.concurrent.Executors\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\main\\java\\org\\apache\\flink\\runtime\\checkpoint\\metadata\\MetadataV2V3SerializerBase.java org.apache.flink.runtime.state.KeyGroupRange\n",
      ".\\flink-1.17.1\\flink-tests\\src\\test\\java\\org\\apache\\flink\\test\\scheduling\\PipelinedRegionSchedulingITCase.java org.apache.flink.runtime.io.network.partition.ResultPartitionType\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_out = random.sample(output, 10)\n",
    "\n",
    "for x,y in rand_out:\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse fails: 311\n",
      "Retry fails: 0\n",
      "Output length: 126518\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parse fails: {parse_fails}\")\n",
    "print(f\"Retry fails: {retry_fails}\")\n",
    "print(f\"Output length: {len(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Clean up, write to TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120269 uniques\n"
     ]
    }
   ],
   "source": [
    "# clean duplicates\n",
    "output = sorted(set(output))\n",
    "print(f\" {len(output)} uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\flink-1.17.1\\flink-core\\src\\main\\java\\org\\apache\\flink\\api\\common\\typeutils\\base\\IntValueComparator.java java.io.IOException\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\main\\java\\org\\apache\\flink\\runtime\\io\\disk\\FileBasedBufferIterator.java java.io.FileNotFoundException\n",
      ".\\flink-1.17.1\\flink-java\\src\\test\\java\\org\\apache\\flink\\api\\java\\operators\\translation\\UnionTranslationTest.java org.apache.flink.api.common.operators.Union\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-code-splitter\\target\\generated-sources\\antlr4\\org\\apache\\flink\\table\\codesplit\\JavaLexer.java org.antlr.v4.runtime.atn.*\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-runtime\\src\\test\\java\\org\\apache\\flink\\table\\runtime\\operators\\join\\RandomSortMergeOuterJoinTest.java org.apache.flink.streami\n",
      ".\\flink-1.17.1\\flink-runtime\\src\\test\\java\\org\\apache\\flink\\runtime\\state\\metrics\\LatencyTrackingStateTestBase.java org.apache.flink.configuration.StateBackendOptions\n",
      ".\\flink-1.17.1\\flink-optimizer\\src\\main\\java\\org\\apache\\flink\\optimizer\\dag\\SingleInputNode.java org.apache.flink.api.common.operators.SingleInputOperator\n",
      ".\\flink-1.17.1\\flink-tests\\src\\test\\java\\org\\apache\\flink\\test\\checkpointing\\UdfStreamOperatorCheckpointingITCase.java org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator\n",
      ".\\flink-1.17.1\\flink-streaming-java\\src\\main\\java\\org\\apache\\flink\\streaming\\runtime\\io\\checkpointing\\BarrierHandlerState.java org.apache.flink.runtime.checkpoint.channel.InputChannelInfo\n",
      ".\\flink-1.17.1\\flink-table\\flink-table-planner\\src\\main\\java\\org\\apache\\flink\\table\\planner\\plan\\nodes\\exec\\common\\CommonExecTableSourceScan.java org.apache.flink.api.common.io.InputFormat\n"
     ]
    }
   ],
   "source": [
    "# pick random 10 to see what output looks like\n",
    "import random\n",
    "\n",
    "rand_out = random.sample(output, 10)\n",
    "\n",
    "for x,y in rand_out:\n",
    "    print(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Map (length & head(10))\n",
      "Size: 13194\n",
      "org.apache.flink.FlinkVersion -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\FlinkVersion.java\n",
      "org.apache.flink.annotation.Experimental -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\Experimental.java\n",
      "org.apache.flink.annotation.Internal -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\Internal.java\n",
      "org.apache.flink.annotation.Public -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\Public.java\n",
      "org.apache.flink.annotation.PublicEvolving -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\PublicEvolving.java\n",
      "org.apache.flink.annotation.VisibleForTesting -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\VisibleForTesting.java\n",
      "org.apache.flink.annotation.docs.ConfigGroup -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\docs\\ConfigGroup.java\n",
      "org.apache.flink.annotation.docs.ConfigGroups -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\docs\\ConfigGroups.java\n",
      "org.apache.flink.annotation.docs.Documentation -> .\\flink-1.17.1\\flink-annotations\\src\\main\\java\\org\\apache\\flink\\annotation\\docs\\Documentation.java\n",
      "org.apache.flink.architecture.common.Conditions -> .\\flink-1.17.1\\flink-architecture-tests\\flink-architecture-tests-base\\src\\main\\java\\org\\apache\\flink\\architecture\\common\\Conditions.java\n"
     ]
    }
   ],
   "source": [
    "# generate map of package -> path where it is defined\n",
    "\n",
    "package_to_path_map = {}\n",
    "# walk path\n",
    "for dir_path, _, file_names in os.walk(path):\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith(\".java\"):\n",
    "            package = dir_path.split(\"\\\\java\\\\\")[-1].replace(\"\\\\\", \".\") + \".\" + file_name.replace(\".java\", \"\")\n",
    "            package_to_path_map[package] = dir_path + \"\\\\\" + file_name\n",
    "            \n",
    "print(\"Package Map (length & head(10))\")\n",
    "print(f\"Size: {len(package_to_path_map)}\")\n",
    "for k,v in list(package_to_path_map.items())[:10]:\n",
    "    print(k + \" -> \" + v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output length: 74500\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Experimental.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Internal.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/PublicEvolving.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Public.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/VisibleForTesting.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Internal.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/docs/ConfigGroup.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Internal.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/docs/ConfigGroups.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Internal.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/docs/ConfigGroups.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/docs/ConfigGroup.java\n",
      "flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/docs/Documentation.java flink-1.17.1/flink-annotations/src/main/java/org/apache/flink/annotation/Internal.java\n"
     ]
    }
   ],
   "source": [
    "# apply mapping to depedency list, generate final output (to use for raw ta)\n",
    "final_output = []\n",
    "\n",
    "for source, depdendency in output:\n",
    "    dep_path = package_to_path_map.get(depdendency)\n",
    "    if dep_path is not None:\n",
    "        final_output.append((source[2:].replace(\"\\\\\", \"/\"), dep_path[2:].replace(\"\\\\\", \"/\")))\n",
    "\n",
    "print(f\"Final output length: {len(final_output)}\")\n",
    "for x,y in final_output[:10]:\n",
    "    print(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to TA\n",
    "raw_ta_output = \"./source_raw_ta/llm_dependencies.raw.ta\"\n",
    "\n",
    "with open(raw_ta_output, \"w+\") as f:\n",
    "  f.write(\"FACT TUPLE : \\n\")\n",
    "\n",
    "  unique_file_paths = set(file_path for file_path, _ in final_output)\n",
    "\n",
    "  # first generate all the concrete instances\n",
    "  for file_path in unique_file_paths:\n",
    "    f.write(f\"$INSTANCE {file_path} cFile\\n\")\n",
    "\n",
    "  # now add in all the dependencies\n",
    "  for file_path, dependency in final_output:\n",
    "    f.write(f\"cLinks {file_path} {dependency}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
